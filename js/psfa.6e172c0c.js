(function(t){function e(e){for(var i,o,s=e[0],l=e[1],c=e[2],u=0,p=[];u<s.length;u++)o=s[u],Object.prototype.hasOwnProperty.call(n,o)&&n[o]&&p.push(n[o][0]),n[o]=0;for(i in l)Object.prototype.hasOwnProperty.call(l,i)&&(t[i]=l[i]);d&&d(e);while(p.length)p.shift()();return r.push.apply(r,c||[]),a()}function a(){for(var t,e=0;e<r.length;e++){for(var a=r[e],i=!0,s=1;s<a.length;s++){var l=a[s];0!==n[l]&&(i=!1)}i&&(r.splice(e--,1),t=o(o.s=a[0]))}return t}var i={},n={psfa:0},r=[];function o(e){if(i[e])return i[e].exports;var a=i[e]={i:e,l:!1,exports:{}};return t[e].call(a.exports,a,a.exports,o),a.l=!0,a.exports}o.m=t,o.c=i,o.d=function(t,e,a){o.o(t,e)||Object.defineProperty(t,e,{enumerable:!0,get:a})},o.r=function(t){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(t,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(t,"__esModule",{value:!0})},o.t=function(t,e){if(1&e&&(t=o(t)),8&e)return t;if(4&e&&"object"===typeof t&&t&&t.__esModule)return t;var a=Object.create(null);if(o.r(a),Object.defineProperty(a,"default",{enumerable:!0,value:t}),2&e&&"string"!=typeof t)for(var i in t)o.d(a,i,function(e){return t[e]}.bind(null,i));return a},o.n=function(t){var e=t&&t.__esModule?function(){return t["default"]}:function(){return t};return o.d(e,"a",e),e},o.o=function(t,e){return Object.prototype.hasOwnProperty.call(t,e)},o.p="/";var s=window["webpackJsonp"]=window["webpackJsonp"]||[],l=s.push.bind(s);s.push=e,s=s.slice();for(var c=0;c<s.length;c++)e(s[c]);var d=l;r.push([3,"chunk-vendors"]),a()})({"030b":function(t,e,a){t.exports=a.p+"media/psfa-p2-cmp_results.81dc6a46.mp4"},"13d3":function(t,e,a){t.exports=a.p+"media/psfa-p1-overview.8fa02a70.mp4"},3:function(t,e,a){t.exports=a("c225")},"402c":function(t,e,a){"use strict";var i=a("2b0e"),n=a("f309");i["a"].use(n["a"]),e["a"]=new n["a"]({})},8240:function(t,e,a){t.exports=a.p+"media/psfa-p3-ablation.5325b434.mp4"},c225:function(t,e,a){"use strict";a.r(e);var i=a("2b0e"),n=function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("v-app",{attrs:{id:"inspire"}},[a("v-app-bar",{attrs:{app:"","clipped-right":"",color:"blue-grey",dark:""}},[a("v-toolbar-title",[t._v("Research Project")]),a("v-spacer")],1),a("v-main",[a("v-container",{staticClass:"fill-height",attrs:{fluid:""}},[a("Article")],1)],1),a("v-footer",{staticClass:"white--text",attrs:{app:"",color:"blue-grey"}},[a("span",[t._v("Yujin Chai")]),a("v-spacer"),a("span",[t._v("Â© "+t._s((new Date).getFullYear()))])],1)],1)},r=[],o=function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("v-row",{attrs:{justify:"center"}},[i("v-col",[i("div",{style:"max-width:800px; margin: 0 auto; font-size: "+t.page_scale+"em"},[i("div",{staticClass:"post-header"},[i("h1",{staticClass:"post-title"},[t._v(" Personalized Audio-Driven 3D Facial Animation"),i("br"),t._v("via Style-Content Disentanglement ")]),i("h3",{staticClass:"post-author"},[t._v(" Yujin Chai, Tianjia Shao, Yanlin Weng, and Kun Zhou ")]),i("p",{staticClass:"post-department"},[t._v(" State Key Lab of CAD&CG, Zhejiang University, China"),i("br")])]),i("div",{staticClass:"post-section"},[i("h2",[t._v("Abstract")]),i("p",[t._v(" We present a learning-based approach for generating 3D facial animations with the motion style of a specific subject from arbitrary audio inputs. The subject style is learned from a video clip (1-2 minutes) either downloaded from the Internet or captured through an ordinary camera. Traditional methods often require many hours of the subject's video to learn a robust audio-driven model and are thus unsuitable for this task. Recent research efforts aim to train a model from video collections of a few subjects but ignore the discrimination between the subject style and underlying speech content within facial motions, leading to inaccurate style or articulation. To solve the problem, we propose a novel framework that disentangles subject-specific style and speech content from facial motions. The disentanglement is enabled by two novel training mechanisms. One is two-pass style swapping between two random subjects, and the other is joint training of the decomposition network and audio-to-motion network with a shared decoder. After training, the disentangled style is combined with arbitrary audio inputs to generate stylized audio-driven 3D facial animations. Compared with start-of-the-art methods, our approach achieves better results qualitatively and quantitatively, especially in difficult cases like bilabial plosive and bilabial nasal phonemes. ")]),i("h2",[t._v("Supplementary Video Part 1 (Overview & Method)")]),i("div",{staticStyle:{margin:"0 auto","text-align":"center"}},[i("video",{staticStyle:{width:"100%"},attrs:{controls:""}},[i("source",{attrs:{src:a("13d3")}})])]),i("h2",[t._v("Supplementary Video Part 2 (Comparison & Results)")]),i("div",{staticStyle:{margin:"0 auto","text-align":"center"}},[i("video",{staticStyle:{width:"100%"},attrs:{controls:""}},[i("source",{attrs:{src:a("030b")}})])]),i("h2",[t._v("Supplementary Video Part 3 (Ablation Study)")]),i("div",{staticStyle:{margin:"0 auto","text-align":"center"}},[i("video",{staticStyle:{width:"100%"},attrs:{controls:""}},[i("source",{attrs:{src:a("8240")}})])])]),i("div",{staticClass:"post-section"},[i("h2",[t._v(" Source Code ")]),i("v-list",{staticStyle:{width:"100%",margin:"0 auto"}},[i("v-list-item",[i("v-list-item-action",{staticStyle:{margin:"0 1.0em 0 0"}},[i("svg",{attrs:{height:36*t.page_scale,viewBox:"0 0 16 16",version:"1.1",width:"32","aria-hidden":"true"}},[i("path",{attrs:{"fill-rule":"evenodd",d:"M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"}})])]),i("v-list-item-content",{staticClass:"pending"},[t._v(" GitHub Repository "),i("br"),t._v(" with Pre-trained Model (TBD) ")])],1)],1)],1)])])],1)},s=[],l={data:()=>({page_scale:1}),created(){window.addEventListener("resize",this.resizeHandler),this.resizeHandler()},destroyed(){window.removeEventListener("resize",this.resizeHandler)},methods:{resizeHandler:function(){let t=window.innerWidth;this.page_scale=Math.min(t,550)/550}}},c=l,d=(a("e6ed"),a("2877")),u=a("6544"),p=a.n(u),h=a("62ad"),f=a("8860"),v=a("da13"),m=a("1800"),b=a("5d23"),y=a("0fd9"),g=Object(d["a"])(c,o,s,!1,null,null,null),w=g.exports;p()(g,{VCol:h["a"],VList:f["a"],VListItem:v["a"],VListItemAction:m["a"],VListItemContent:b["a"],VRow:y["a"]});var _={props:{},components:{Article:w},data:()=>({drawer:null,drawerRight:null,right:!1,left:!1})},j=_,C=a("7496"),S=a("40dc"),x=a("a523"),V=a("553a"),O=a("f6c4"),P=a("2fa4"),A=a("2a7f"),T=Object(d["a"])(j,n,r,!1,null,null,null),z=T.exports;p()(T,{VApp:C["a"],VAppBar:S["a"],VContainer:x["a"],VFooter:V["a"],VMain:O["a"],VSpacer:P["a"],VToolbarTitle:A["a"]});var k=a("402c");i["a"].config.productionTip=!1,new i["a"]({vuetify:k["a"],render:function(t){return t(z)}}).$mount("#app")},d0c0:function(t,e,a){},e6ed:function(t,e,a){"use strict";var i=a("d0c0"),n=a.n(i);n.a}});
//# sourceMappingURL=psfa.6e172c0c.js.map